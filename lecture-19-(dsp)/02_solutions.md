## ğŸ”· **Pattern 1: Query Optimization & Connection Pool Implementation**

### ğŸ”§ Whatâ€™s Done:

* **Cache static/frequently accessed data** (e.g., booking/payment history, profiles).
* **Database redundancy** or **NoSQL** for faster reads of non-transactional data.
* **Connection pooling**: Reuse database connections to reduce overhead.
* Allow **multiple threads to share the same DB connection**.

### ğŸ’¡ Benefit:

* Lightweight, cost-effective fixes.
* Improved performance without major infrastructure changes.
* Suitable for growth to 1â€“2 cities (\~100 bookings/min).

## ğŸ”· **Pattern 2: Vertical Scaling (Scale-Up)**

### ğŸ”§ Whatâ€™s Done:

* **Upgrade existing machine hardware**: More RAM, better SSD.
* Still using **one big machine**, just more powerful.

### âš ï¸ Limitation:

* **Only scales to a point**; eventually becomes **too expensive**.
* A single point of failure remains.
* Still good for moderate growth (\~3 cities, 300 bookings/min).

### ğŸ’¡ Benefit:

* Simple to implement.
* Good **short-term** fix for performance issues.

## ğŸ”· **Pattern 3: Command Query Responsibility Segregation (CQRS)**

### ğŸ”§ Whatâ€™s Done:

* **Split Read and Write operations**:

  * **Primary machine handles Writes**.
  * **Replicas handle Reads**.
* Add **2 replica machines** for reads.
* Use **replication lag** to minimize stale reads.

### âš ï¸ Limitation:

* **Write bottleneck** still exists (writes go only to primary).
* **Lag between primary and replicas** affects **consistency** and **user experience**.

### ğŸ’¡ Benefit:

* Great for **read-heavy** workloads.
* Helps handle **high traffic**, but not ideal for frequent writes.

## ğŸ”· **Pattern 4: Multi-Primary Replication (Multi-Master)**

### ğŸ”§ Whatâ€™s Done:

* **All machines are primaries** â€“ any machine can **read and write**.
* Works in a **logical circular ring** (each node syncs with others).
* **Data can be written to any node**, and other nodes get updated.
* Read from the fastest node (low latency).

### âš ï¸ Limitation:

* **Conflict resolution** becomes complex (what if two primaries write the same record?).
* **Latency in replication**.
* System again slows down with growth (\~50 req/sec even after scaling to 5 more cities).

### ğŸ’¡ Benefit:

* Maximizes parallelism.
* No single point of write.
* Great for **distributed global applications**â€”but **complex to maintain**.

## ğŸ“Œ Summary Table

| Pattern | Strategy                     | Scaling Type | Best For                       | Weaknesses                      |
| ------- | ---------------------------- | ------------ | ------------------------------ | ------------------------------- |
| 1       | Caching, Pooling, Redundancy | Optimization | Initial growth (\~100 req/min) | Limited long-term benefit       |
| 2       | Bigger Machine (RAM, SSD)    | Vertical     | Moderate traffic               | Expensive, not scalable forever |
| 3       | CQRS (Separate Read/Write)   | Horizontal   | Read-heavy apps                | Write bottleneck, lag issues    |
| 4       | Multi-Primary Replication    | Horizontal   | High availability, writes      | Conflict resolution, complexity |


---


## ğŸ”· **Pattern 5: Partitioning of Data by Functionality**

### ğŸ”§ Whatâ€™s Done:

* Break your data **based on business function**.

  * Example: Separate location data, user data, booking history into **different databases or schemas**.
* Each database can have its **own scaling pattern** (e.g., primary-replica, multi-primary).
* The **backend/application layer** is responsible for fetching and combining results.

### âœ… Advantages:

* Easier to manage and **scale per functionality**.
* Better **fault isolation** (a crash in the location DB wonâ€™t crash the booking DB).
* Modular expansion (e.g., add a payments DB later).

### âš ï¸ Challenges:

* Increases **application logic complexity** (needs cross-DB joins or aggregations).
* **Data consistency** becomes harder across services.

### ğŸ’¡ Best For:

* Apps expanding in **features or domains** (e.g., entering international markets).

## ğŸ”· **Pattern 6: Horizontal Scaling or Scale-Out (Sharding)**

### ğŸ”§ Whatâ€™s Done:

* **Sharding** = Split data **horizontally** across machines.

  * E.g., user ID 1â€“1M goes to Shard A, 1Mâ€“2M to Shard B.
* Each machine holds **only part of the data**, but all share the **same DB schema**.
* Machines can have **replicas for high availability**.

### âœ… Advantages:

* **Highly scalable** â€” each shard handles only a subset of load.
* Better fault tolerance.
* Shards can be **geographically distributed**.

### âš ï¸ Challenges:

* **Complex to implement** (routing, cross-shard queries).
* Requires **shard key design** and **load balancing**.
* More difficult to manage joins or analytics across shards.

### ğŸ’¡ Best For:

* **Very large-scale apps** expanding across cities or countries.

## ğŸ”· **Pattern 7: Data Centre Wise Partition (Geographical Partitioning)**

### ğŸ”§ Whatâ€™s Done:

* Deploy **data centers across continents**.
* Each data center handles **local traffic** to reduce **latency**.
* **Cross-data center replication** is enabled for **disaster recovery** and **consistency**.

### âœ… Advantages:

* Ensures **high availability** even if one region goes down.
* Drastically reduces **latency** for users in different geographies.
* Helps meet **compliance** (e.g., GDPR mandates local data storage).

### âš ï¸ Challenges:

* Complex **data replication** setup.
* Risk of **data inconsistency** between regions.
* Requires **DNS-level routing** and IP planning.

### ğŸ’¡ Best For:

* **Global businesses** or services with users across countries/continents.

## ğŸ“Œ Summary Table (for Patterns 5â€“7)

| Pattern | Strategy                | Best For                        | Challenges                      |
| ------- | ----------------------- | ------------------------------- | ------------------------------- |
| 5       | Functional Partitioning | Modular apps, multiple domains  | Complex app logic, data joining |
| 6       | Sharding (Scale-Out)    | Large-scale, high-traffic apps  | Hard to implement and maintain  |
| 7       | Geo-Based Partitioning  | Global scale, disaster recovery | High complexity, sync issues    |

### âœ… Final Takeaway:

As your app scales:

* **Start simple** (Pattern 1 & 2),
* Move to **read/write separation** (Pattern 3),
* Eventually adopt **sharding and geo-partitioning** (Pattern 6 & 7) for true global scalability.
